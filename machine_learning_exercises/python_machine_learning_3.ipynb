{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "omvoE2AiTVb8"
   },
   "outputs": [],
   "source": [
    "# Printing all outputs in a cell (not only the last output)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVmuMwCqFuJT"
   },
   "source": [
    "## ** 머신러닝 3주차 실습 (Due 6/15 10pm) **\n",
    "\n",
    "### Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVmuMwCqFuJT"
   },
   "source": [
    "---\n",
    "#### ** 1) [심층 신경망의 구성과 자동화] ** \n",
    "   \n",
    "* (입력층: 노드수), (은닉층: 은닉층의 수, 각 은닉층의 노드수, 각 은닉층의 활성화 함수 종류), (출력층: 노드수, 출력층의 활성화 함수 종류)를\n",
    "  기본적인 신경망 구성의 파라메터로 받아서, 복수의 은닉층을 가진 신경망을 편리하게 구성해주는 클래스를 작성해보자.  \n",
    "  \n",
    "* 이때 각 층에서 선택 가능한 활성화 함수로써, 은닉층에는 (sigmoid, tanh, ReLU), 출력층에는 (identity, sigmoid, softmax)의 \n",
    "  자유로운 선택이 가능하도록 신경망 클래스를 확장해보고, 층층이 다른 활성화 함수를 사용할 경우 이에 걸맞는 역전파가 가능하도록 train 함수를 확장해보자. \n",
    "\n",
    "* 더 나아가, 일반적인 활성화 함수의 사용이 가능하도록, 수치미분함수를 도입하여 역전파 및 train 함수를 작성해보자.\n",
    "\n",
    "* 가중치의 초기값분포에 따라서 각 층에서의 출력분포가 어떻게 달라지는지 확인하여보자.\n",
    "\n",
    "* 학습한 모형의 파라메터들을 저장하고, 언제든지 다시 로드하여 훈련된 신경망 모형을 활용할 수 있도록 구성해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기 (MNIST 손글씨 데이터)\n",
    "\n",
    "# 훈련데이터\n",
    "training_data_file = open(\"mnist_train.csv\",'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# 테스트데이터\n",
    "test_data_file = open(\"mnist_test.csv\",'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_list)\n",
    "len(test_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# NNfactory.py 임포트\n",
    "import NNfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * 다음과 같은 구조의 다층퍼셉트론 연결이 초기화 되었습니다 *\n",
      "\n",
      " > 모델이름 = myMLP\n",
      " > 총 층수 (입력 + 은닉(s) + 출력) =  4\n",
      " > 각 층에서의 노드수 =  [784, 100, 100, 10]\n",
      " > 각 층에서의 활성화 함수 =  ['identity', 'tanh', 'tanh', 'softmax']\n",
      " > 학습률(Learning Rate) =  0.005\n"
     ]
    }
   ],
   "source": [
    "name = 'myMLP'\n",
    "structure = '784:identity|100:tanh|100:tanh|10:softmax'\n",
    "mynn = NNfactory.MLP(model_structure = structure, model_nametag = name, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      " * epoch = 1\n",
      " [====================] 100%\n",
      " > 훈련 샘플 학습도 (정확도 & 평균에러) \n",
      " \n",
      " * 현재 정확도  =  0.894710528947  | (정답수)/(테스트 데이터수) =  8948 / 10001\n",
      " * 현재 평균에러 =  0.0160944238297\n",
      "\n",
      " > 테스트 샘플 학습도 (정확도 & 평균에러) \n",
      " \n",
      " * 현재 정확도  =  0.8813  | (정답수)/(테스트 데이터수) =  8813 / 10000\n",
      " * 현재 평균에러 =  0.0182080230061\n",
      "\n",
      "\n",
      " * epoch = 2\n",
      " [================    ] 80%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4d124c954dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtarget_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmynn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/02_PPSML/00_lecture_note/NNfactory.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_list, target_list)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/02_PPSML/00_lecture_note/NNfactory.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, target_list)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdE_over_df_at_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_over_da\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_over_da\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# 네트웍의 훈련: the neural network\n",
    "data_type = 'mnist'\n",
    "\n",
    "# 최대 학습 주기 설정\n",
    "epochs = 100\n",
    "\n",
    "n_data_tot=len(training_data_list)\n",
    "n_data_max=60000  # 훈련에 사용할 데이터 갯수 (최대 60000)\n",
    "n_data_test=min(int((n_data_tot-n_data_max)*0.7),int(n_data_max*0.7))\n",
    "n_data = len(training_data_list[:n_data_max])\n",
    "dn_data = int(n_data/20)\n",
    "print (dn_data)\n",
    "\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    print(' * epoch = {}'.format(e+1))\n",
    "    id_data = 0\n",
    "    \n",
    "    for record in training_data_list[:n_data_max]:\n",
    "        \n",
    "        id_data += 1\n",
    "        if (id_data%dn_data==0):\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(' [%-20s] %d%%' % ( '='*(id_data//dn_data), 5*(id_data//dn_data)))\n",
    "            sys.stdout.flush()\n",
    "            sleep(0.25)\n",
    "        \n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "\n",
    "        # 입력데이터 ['x1','x2',...] & 입력데이터의 스케일링 (n_input_nodes, )\n",
    "        if data_type == 'mnist':\n",
    "            input_list = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        else:\n",
    "            input_list = np.asfarray(all_values[1:])\n",
    "\n",
    "    \n",
    "        # create the target output values (shape = (10,))\n",
    "        target_list = np.zeros(10) #mynn.n_nodes[-1])\n",
    "        \n",
    "        # all_values[0] is the target label for this record\n",
    "        target_list[int(all_values[0])] = 1.0\n",
    "\n",
    "        mynn.train(input_list, target_list)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    print('')\n",
    "    print(' > 훈련 샘플에 대한 성능 (정확도 & 평균에러) ')\n",
    "    mynn.check_accuracy_error(training_data_list, 0, n_data_max, data_type='mnist')\n",
    "    print('')\n",
    "    print(' > 테스트 샘플에 대한 성능 (정확도 & 평균에러) ')\n",
    "    mynn.check_accuracy_error(test_data_list, 0, len(test_data_list), data_type='mnist')\n",
    "        \n",
    "    print('\\n')\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련한 모형의 저장\n",
    "\n",
    "훈련된 모형은 NNfactory클래스 안의 save_model메소드를 사용하여 .npy포맷의 numpy array로 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynn.save_model(fname='mymodel.npy', nametag='first run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 저장된 모형 불러오기 :\n",
    "\n",
    "방법1) 껍데기 신경망 인스턴스를 선언하고, 그 인스턴스의 load_model메소드를 이용하여 저장된 .npy파일로부터 신경망을 직접 로드한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * 다음과 같은 구조의 다층퍼셉트론 연결이 초기화 되었습니다 *\n",
      "\n",
      " > 모델이름 = a MLP\n",
      " > 총 층수 (입력 + 은닉(s) + 출력) =  3\n",
      " > 각 층에서의 노드수 =  [784, 100, 10]\n",
      " > 각 층에서의 활성화 함수 =  ['identity', 'tanh', 'softmax']\n",
      " > 학습률(Learning Rate) =  0.005\n"
     ]
    }
   ],
   "source": [
    "mynn2 = NNfactory.MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * 다음과 같은 구조의 다층퍼셉트론 모형이 \"load_model_np\" 정보로부터 로드되었습니다. *\n",
      "\n",
      " > 모델이름 = first run\n",
      " > 총 층수 (입력 + 은닉(s) + 출력) =  4\n",
      " > 각 층에서의 노드수 =  [784, 100, 100, 10]\n",
      " > 각 층에서의 활성화 함수 =  ['identity', 'tanh', 'tanh', 'softmax']\n",
      " > 학습률(Learning Rate) =  0.005\n"
     ]
    }
   ],
   "source": [
    "mynn2.load_model(fname='mymodel.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 저장된 모형 불러오기 :\n",
    "\n",
    "방법2) 저장된 .npy파일로부터 신경망정보가 담긴 넘파이 배열을 직접 로드하고, 이 넘파이 배열을 새 신경망 인스턴스 생성에 사용하여 저장된 모형과 똑같은 신경망을 로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * 다음과 같은 구조의 다층퍼셉트론 모형이 \"load_model_np\" 정보로부터 로드되었습니다. *\n",
      "\n",
      " > 모델이름 = first run\n",
      " > 총 층수 (입력 + 은닉(s) + 출력) =  4\n",
      " > 각 층에서의 노드수 =  [784, 100, 100, 10]\n",
      " > 각 층에서의 활성화 함수 =  ['identity', 'tanh', 'tanh', 'softmax']\n",
      " > 학습률(Learning Rate) =  0.005\n"
     ]
    }
   ],
   "source": [
    "mymodel = np.load('mymodel.npy')\n",
    "mynn3 = NNfactory.MLP(load_model_np=mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = training_data_list[0].split(',')\n",
    "data[0]\n",
    "input_list = np.asfarray(data[1:])/255*0.99 + 0.01\n",
    "# input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로드된 모형(mynn2)의 순방향 출력 체크 (첫번째 데이터에 대하여) \n",
    "np.argmax(mynn2.feedforward(input_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ** 2) [과적합(Overfititng)의 탐지 및 방지] ** \n",
    "   \n",
    "* 주어진 난수 데이터에 대하여, 은닉층의 수가 늘어남에 따라서 학습 곡선이 어떻게 달라지는지 체크해보자.\n",
    "   \n",
    "* 머신러닝 2주차의 회귀 모형의 신경망 학습에 대하여 과적합이 일어난 상황을 만들고, 이를 확인해보자.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "python_matplotlib.ipynb",
   "provenance": [
    {
     "file_id": "1wHHu3Ia0i4uzibY_CbZhfH-URG7tnL1A",
     "timestamp": 1526544642454
    },
    {
     "file_id": "1yW86LYjSGJJpGr54zaTOCMxuOaPxGo8j",
     "timestamp": 1526540479681
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
